{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model Using AWS SageMaker\n",
    "\n",
    "In this notebook, I will train the COMPAS `XGBoost` model using SageMaker.\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Loading the data to S3 buckets\n",
    "2. Setting the model\n",
    "3. Hyperparameter tuning\n",
    "4. Evaluating the model\n",
    "5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data to S3 buckets\n",
    "\n",
    "In the `Data-Exploration.ipynb` notebook, I've loaded and prepared the data. Also, I've splitted into training (75%) and testing (25%) data using the Scikit-Learn's `train_test_split()` given a random seed generator. Finally, I've exported all the data into `.csv` files in the `data` folder.\n",
    "\n",
    "Now, I will create a S3 bucket and upload the `.csv` files to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create an S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the `data_dir` where you've saved your `.csv` files. Decide on a descriptive `prefix` that defines where your data will be uploaded in the default S3 bucket. \n",
    "\n",
    "Finally, create a pointer to your training data by calling `sagemaker_session.upload_data` and passing in the required parameters. It may help to look at the [Session documentation](https://sagemaker.readthedocs.io/en/stable/session.html#sagemaker.session.Session.upload_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'upload_data' method will be deprecated in favor of 'S3Uploader' class (https://sagemaker.readthedocs.io/en/stable/s3.html#sagemaker.s3.S3Uploader) in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "\n",
    "# set prefix, a descriptive name for a directory  \n",
    "prefix = 'sagemaker/compas_model'\n",
    "\n",
    "# upload all data to S3\n",
    "input_data = sagemaker_session.upload_data(path=data_dir,\n",
    "                                           bucket=bucket,\n",
    "                                           key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you go to the `AWS Console` and down to `S3 Management Console`, you shall see a S3 bucket named something like `sagemaker-us-east-######`. It is inside this S3 that you'll find your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the XGBoost Model\n",
    "\n",
    "Now that I have the training, validation and test data uploaded to S3, I can construct the `XGBoost` model and train it. I will use SageMaker's hyperparameter tuning functionality to train multiple models and use the one that performs the best on the validation set.\n",
    "\n",
    "Since, in the COMPAS context, I am concerned in reducing the false positive rate while keeping a good accuracy, I will tune the model to maximize the `validation:map` which means *Mean Average Precision*. I do not want that the model label someone (either black/white) as medium/high risk for recidivism if the ground truth is low.\n",
    "\n",
    "To begin with, I will need to construct an `estimator` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.amazon.amazon_estimator:'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n",
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# construct the image name for the training container.\n",
    "container = get_image_uri(sagemaker_session.boto_region_name, 'xgboost')\n",
    "\n",
    "# Now that I know which container to use, I can construct the estimator object.\n",
    "xgb = sagemaker.estimator.Estimator(container, # The name of the training container\n",
    "                                    role,      # The IAM role to use (our current role in this case)\n",
    "                                    train_instance_count=5,  # The number of instances to use for training\n",
    "                                    train_instance_type='ml.m4.xlarge',  # The type of instance ot use for training\n",
    "                                    output_path=f\"s3://{sagemaker_session.default_bucket()}/{prefix}/output\",  # Where to save the output (the model artifacts)\n",
    "                                    sagemaker_session=sagemaker_session)  # The current SageMaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beginning the hyperparameter tuning, make sure to set any model specific hyperparameters that I wish to have default values. There are quite a few that can be set when using the XGBoost algorithm, below are just a few of them. If you would like to change the hyperparameters below or modify additional ones you can find additional information on the [XGBoost hyperparameter page](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html).\n",
    "\n",
    "Also, note that since the COMPAS model is a binary classifier, I will use the objetive as `'reg:logistic'` instead of the default `'reg:squarederror'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        objective='reg:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Tuning\n",
    "\n",
    "Now that I have my `estimator` object completely set up, it is time to create the `hyperparameter tuner`. To do this I need to construct a new object which contains each of the parameters I want SageMaker to tune. In this case, I wish to find the best values for the `max_depth`, `eta`, `min_child_weight`, `subsample`, and `gamma` parameters. Note that for each parameter that I want SageMaker to tune I need to specify both the *type* of the parameter and the *range* of values that parameter may take on.\n",
    "\n",
    "In addition, I specify the number of models to construct (`max_jobs`) and the number of those that can be trained in parallel (`max_parallel_jobs`). In the cell below I have chosen to train 20 models, of which I ask that SageMaker train 5 at a time in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgb,  # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'validation:map',  # The metric used to compare trained models.\n",
    "                                               objective_type = 'Maximize',  # Whether I wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 20,  # The total number of models to train\n",
    "                                               max_parallel_jobs = 5,  # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'max_depth': IntegerParameter(3, 12),\n",
    "                                                    'eta'      : ContinuousParameter(0.05, 0.5),\n",
    "                                                    'min_child_weight': IntegerParameter(2, 8),\n",
    "                                                    'subsample': ContinuousParameter(0.5, 0.9),\n",
    "                                                    'gamma': ContinuousParameter(0, 10),\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have my `hyperparameter tuner` object completely set up, it is time to train it. To do this I make sure that SageMaker knows our input data is in `.csv` format and then execute the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-2-349061725184/sagemaker/compas_model'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# This is a wrapper around the location of our train and validation data,\n",
    "# to make sure that SageMaker knows our data is in csv format.\n",
    "\n",
    "import os\n",
    "\n",
    "train_location = os.path.join(input_data, 'train.csv')\n",
    "val_location = os.path.join(input_data, 'val.csv')\n",
    "test_location = os.path.join(input_data, 'X_test.csv')\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')\n",
    "\n",
    "xgb_hyperparameter_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you go to the `AWS Console` and down to `Amazon SageMaker`, you shall see under `Training > Training jobs` tab, the training jobs being performed in $n$ instances that you designated in `train_instance_count` when creating the `estimator` object.\n",
    "\n",
    "The .`fit()` method takes care of setting up and fitting a number of different models, each with different hyperparameters. If you wish to wait for this process to finish, you can call the `.wait()` method or monitor in the `Training Jobs` tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "xgb_hyperparameter_tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the hyperamater tuner has finished, I can retrieve information about the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-200606-1423-001-117c694a'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_hyperparameter_tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, since I'd like to set up a batch transform job to test the best model, I can construct a new `estimator` object from the results of the best training job. The `xgb_attached` object below can now be used as though I constructed an `estimator` with the best performing hyperparameters and then fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-06 14:26:05 Starting - Preparing the instances for training\n",
      "2020-06-06 14:26:05 Downloading - Downloading input data\n",
      "2020-06-06 14:26:05 Training - Training image download completed. Training in progress.\n",
      "2020-06-06 14:26:05 Uploading - Uploading generated training model\n",
      "2020-06-06 14:26:05 Completed - Training job completed\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:25:53:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:25:53:INFO] Setting up HPO optimized metric to be : map\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:25:53:INFO] File size need to be processed in the node: 0.07mb. Available memory size in the node: 8478.98mb\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:25:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[14:25:53] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[14:25:53] 2968x8 matrix with 23744 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:25:53:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[14:25:53] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[14:25:53] 990x8 matrix with 7920 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[0]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-map' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-map hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[1]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[2]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[3]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[4]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[5]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[6]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[7]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[8]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[9]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34m[14:25:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[10]#011train-map:1#011validation-map:1\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[0]#011train-map:1#011validation-map:1\n",
      "\u001b[0m\n",
      "Training seconds: 50\n",
      "Billable seconds: 50\n"
     ]
    }
   ],
   "source": [
    "xgb_attached = sagemaker.estimator.Estimator.attach(xgb_hyperparameter_tuner.best_training_job())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "\n",
    "Now that I have my best performing model, I can test it. To do this I will use the `batch transform` functionality. To start with, I need to build a `transformer` object from my fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer = xgb_attached.transformer(instance_count = 1,\n",
    "                                           instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I ask SageMaker to begin a `batch transform` job using the trained model and applying it to the test data previously stored in S3. I need to make sure to provide SageMaker with the type of data that I am providing to our model, in my case `text/csv`, so that it knows how to serialize the data. In addition, I need to make sure to let SageMaker know how to split our data up into chunks if the entire data set happens to be too large to send to our model all at once.\n",
    "\n",
    "Note that when I ask SageMaker to do this it will execute the `batch transform` job in the background. Since I need to wait for the results of this job before I can continue, I use the `.wait()` method or I monitor the `Batch transform jobs` tab under `Inference` in the `Amazon SageMaker`. An added benefit of this is that I get some output from the batch transform job which lets me know if anything went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer.transform(test_location,\n",
    "                          content_type='text/csv',\n",
    "                          split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................\n",
      "\u001b[32m2020-06-06T14:44:17.110:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2020-06-06 14:44:16 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[34m[2020-06-06 14:44:16 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2020-06-06 14:44:16 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2020-06-06 14:44:16 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[34m[2020-06-06 14:44:17 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:44:17:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[34m[2020-06-06 14:44:17 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:44:17:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[34m[2020-06-06 14:44:17 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:44:17:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:44:17:INFO] Model loaded successfully for worker : 42\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:44:17:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2020-06-06:14:44:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35mArguments: serve\u001b[0m\n",
      "\u001b[35m[2020-06-06 14:44:16 +0000] [1] [INFO] Starting gunicorn 19.7.1\u001b[0m\n",
      "\u001b[35m[2020-06-06 14:44:16 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[35m[2020-06-06 14:44:16 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2020-06-06 14:44:16 +0000] [39] [INFO] Booting worker with pid: 39\u001b[0m\n",
      "\u001b[35m[2020-06-06 14:44:17 +0000] [40] [INFO] Booting worker with pid: 40\u001b[0m\n",
      "\u001b[35m[2020-06-06:14:44:17:INFO] Model loaded successfully for worker : 39\u001b[0m\n",
      "\u001b[35m[2020-06-06 14:44:17 +0000] [41] [INFO] Booting worker with pid: 41\u001b[0m\n",
      "\u001b[35m[2020-06-06:14:44:17:INFO] Model loaded successfully for worker : 40\u001b[0m\n",
      "\u001b[35m[2020-06-06 14:44:17 +0000] [42] [INFO] Booting worker with pid: 42\u001b[0m\n",
      "\u001b[35m[2020-06-06:14:44:17:INFO] Model loaded successfully for worker : 41\u001b[0m\n",
      "\u001b[35m[2020-06-06:14:44:17:INFO] Model loaded successfully for worker : 42\u001b[0m\n",
      "\u001b[35m[2020-06-06:14:44:17:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[35m[2020-06-06:14:44:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the `batch transform job` has finished, the resulting output is stored on S3. Since I wish to analyze the output inside of our notebook I can use a bit of notebook magic to copy the output file from its S3 location and save it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-2-349061725184/xgboost-200606-1423-001-117c694a-2020-06-06-14-40-53-246/X_test.csv.out to sagemaker/compas_model/X_test.csv.out\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well my model, I will compute some metrics between the predicted and actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = pd.read_csv(os.path.join(data_dir, 'X_test.csv.out'), header=None)\n",
    "y_pred = np.where(y_pred_proba.to_numpy() > 0.5, 1, 0)\n",
    "y_test = pd.read_csv(os.path.join(data_dir, 'y_test.csv'), header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 1.0\n",
      "Overall Precision: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall Accuracy: {round(acc, 3)}\")\n",
    "print(f\"Overall Precision: {round(precision, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[864,   0],\n",
       "       [  0, 456]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "With a Amazon SageMaker, I've built a XGBoost model, did hyperparameter tuning and tested the model. This was easy to be done with the Sagemaker AWS Python SDK using the high level API.\n",
    "\n",
    "By specifying the right metric (precision), I've found the hyperparameters to train the best the model for that metric and obtained a whooping 100% accuracy and 100% precision in the COMPAS data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
